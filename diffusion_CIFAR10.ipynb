{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wki9cpQZ7dxN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as opt\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms, datasets\n",
        "from torch.cuda import amp\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5fmBN1VmwjY"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "beta_start = 1e-4\n",
        "beta_end = 0.02\n",
        "steps = 1000\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "image_size = 64\n",
        "image_channel = 3\n",
        "epochs = 300\n",
        "lr = 3e-4\n",
        "weight_decay = 0\n",
        "batch_size = 140\n",
        "num_class = 200\n",
        "pos_dim = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IehB5lv6hgvq"
      },
      "outputs": [],
      "source": [
        "# Constants used for diffusion model\n",
        "beta = torch.linspace(beta_start, beta_end, steps).to(device)\n",
        "sqrt_beta = torch.sqrt(beta).view(-1, 1, 1, 1)\n",
        "alpha = 1 - beta\n",
        "alphas_cumprod = torch.cumprod(alpha, axis=0)\n",
        "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod).view(-1, 1, 1, 1)\n",
        "one_minus_alphas_cumprod = 1 - alphas_cumprod\n",
        "sqrt_one_minus_alphas_cumprod = torch.sqrt(one_minus_alphas_cumprod).view(-1, 1, 1, 1)\n",
        "one_over_sqrt_alpha = 1/torch.sqrt(alpha).view(-1, 1, 1, 1)\n",
        "one_minus_alpha = (1 - alpha).view(-1, 1, 1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sG4RT6IBnDy6"
      },
      "outputs": [],
      "source": [
        "# util.py\n",
        "\n",
        "# Forward pass\n",
        "def forward_pass(images, t):\n",
        "    batch_sqrt_alphas_cumprod = sqrt_alphas_cumprod[t]\n",
        "    batch_sqrt_one_minus_alphas_cumprod = sqrt_one_minus_alphas_cumprod[t]\n",
        "    noise = torch.randn_like(images).to(device)\n",
        "\n",
        "    return batch_sqrt_alphas_cumprod * images + batch_sqrt_one_minus_alphas_cumprod * noise, noise\n",
        "\n",
        "# Sampling\n",
        "def sampling(model, labels, cfg_scale: int = 3):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        x = torch.randn(labels.shape[0], image_channel, image_size, image_size).to(device)\n",
        "\n",
        "        for i in tqdm(range(steps-1, -1, -1)):\n",
        "            t = torch.tensor([i]*labels.shape[0]).to(device)\n",
        "\n",
        "            # Classifier free guidance\n",
        "            predicted_noise_no_label = model(x, t, None)\n",
        "            predicted_noise_with_label = model(x, t, labels)\n",
        "            predicted_noise = torch.lerp(predicted_noise_no_label, predicted_noise_with_label, cfg_scale)\n",
        "\n",
        "            if(i == 0):\n",
        "                noise = torch.zeros_like(x).to(device)\n",
        "            else:\n",
        "                noise = torch.randn_like(x).to(device)\n",
        "\n",
        "            x = one_over_sqrt_alpha[t] * (x - ((one_minus_alpha[t])/(sqrt_one_minus_alphas_cumprod[t]))*predicted_noise) + sqrt_beta[t] * noise\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    x = (x.clamp(-1, 1) + 1) / 2\n",
        "    x = (x * 255).type(torch.uint8)\n",
        "\n",
        "    for i in range(x.shape[0]):\n",
        "        tensor = x[i].permute(1, 2, 0).to(\"cpu\")\n",
        "        plt.imshow(tensor)\n",
        "        plt.show()\n",
        "\n",
        "def positional_embedding(num_step: int, emb_dim: int) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Create positional embedding tensor.\n",
        "\n",
        "    :param num_step: Number of time steps.\n",
        "    :param emb_dim: Embedding dimension.\n",
        "    :return: Positional embedding tensor.\n",
        "    \"\"\"\n",
        "    matrix = torch.zeros(num_step, emb_dim)\n",
        "    for i in range(num_step):\n",
        "        for j in range(0, emb_dim, 2):\n",
        "            matrix[i, j] = np.sin(i/(10000**(j/emb_dim)))\n",
        "            if(j+1<emb_dim):\n",
        "                matrix[i, j+1] = np.cos(i/(10000**(j/emb_dim)))\n",
        "\n",
        "    return matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hT8TE3hV7rZl"
      },
      "outputs": [],
      "source": [
        "# model.py\n",
        "\n",
        "class AdaNorm(nn.Module):\n",
        "    def __init__(self, num_channel: int, channel_per_group: int = 16, emb_dim: int = 1024):\n",
        "        super().__init__()\n",
        "        assert num_channel % channel_per_group == 0, \"num_channel must be divisible by channel_per_group\"\n",
        "        num_group = num_channel // channel_per_group\n",
        "        self.embedding_proj = nn.Sequential(\n",
        "            nn.Linear(emb_dim, 2 * num_channel),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.gnorm = nn.GroupNorm(num_group, num_channel, affine=False)\n",
        "\n",
        "    def forward(self, tensor: torch.Tensor, embedding: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Perform adanormalization on input tensor.\n",
        "\n",
        "        :param tensor: Input tensor to be normalized.\n",
        "        :param embedding: Embedding tensor containing time embedding and potentially class embedding.\n",
        "        :return: Normalized tensor.\n",
        "        \"\"\"\n",
        "        embedding = self.embedding_proj(embedding)\n",
        "        embedding = embedding.view(embedding.shape[0], embedding.shape[1], 1, 1)\n",
        "        scale, shift = torch.chunk(embedding, 2, dim=1)\n",
        "\n",
        "        tensor = self.gnorm(tensor)\n",
        "        tensor = tensor * torch.sigmoid(scale) + shift\n",
        "        return tensor\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_channel: int, out_channel: int, emb_dim: int = 1024, up: bool = False, down: bool = False, channel_per_group: int = 16):\n",
        "        super().__init__()\n",
        "\n",
        "        # Upsampling or downsampling only for skip connection\n",
        "        self.up = up\n",
        "        self.down = down\n",
        "\n",
        "        # Normalization layers\n",
        "        self.norm1 = AdaNorm(in_channel, emb_dim=emb_dim)\n",
        "        self.norm2 = AdaNorm(out_channel, emb_dim=emb_dim)\n",
        "\n",
        "        # Convolution layers\n",
        "        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(out_channel, out_channel, kernel_size=3, padding=1)\n",
        "\n",
        "        # Skip connection\n",
        "        if in_channel != out_channel or up or down:\n",
        "            self.skip_connection = nn.Sequential(\n",
        "                nn.Conv2d(in_channel, out_channel, kernel_size=1),\n",
        "                nn.Upsample(scale_factor=2) if up else nn.Identity(),\n",
        "                nn.AvgPool2d(kernel_size=2) if down else nn.Identity(),\n",
        "            )\n",
        "        else:\n",
        "            self.skip_connection = nn.Identity()\n",
        "\n",
        "    def forward(self, tensor: torch.Tensor, embedding: torch.Tensor) -> torch.Tensor:\n",
        "        skip_tensor = self.skip_connection(tensor)\n",
        "\n",
        "        # Main path\n",
        "        tensor = self.norm1(tensor, embedding)\n",
        "        tensor = F.relu(tensor)\n",
        "        if self.up:\n",
        "            tensor = F.interpolate(tensor, scale_factor=2)\n",
        "        if self.down:\n",
        "            tensor = F.avg_pool2d(tensor, kernel_size=2)\n",
        "        tensor = self.conv1(tensor)\n",
        "        tensor = self.norm2(tensor, embedding)\n",
        "        tensor = F.relu(tensor)\n",
        "        tensor = self.conv2(tensor)\n",
        "\n",
        "        tensor += skip_tensor\n",
        "        return tensor\n",
        "\n",
        "\n",
        "class SelfAttentionBlock(nn.Module):\n",
        "    def __init__(self, in_channel: int, image_size: int, patch_size: int = 1, head_dim: int = 32, channel_per_group: int = 16):\n",
        "        super().__init__()\n",
        "        scaling_factor: int = patch_size ** 2\n",
        "        embedding_dim: int = in_channel * scaling_factor\n",
        "        self.head_dim: int = head_dim\n",
        "        self.num_head: int = embedding_dim // head_dim\n",
        "        self.scale: float = head_dim ** -0.5\n",
        "        self.num_token = (image_size // patch_size) ** 2\n",
        "        self.gnorm1 = nn.GroupNorm(in_channel // channel_per_group, in_channel)\n",
        "        self.gnorm2 = nn.GroupNorm(embedding_dim // channel_per_group, embedding_dim)\n",
        "        self.gnorm3 = nn.GroupNorm(embedding_dim // channel_per_group, embedding_dim)\n",
        "\n",
        "        # Patch and unpatch image\n",
        "        self.patch = nn.Conv2d(in_channel, embedding_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.unpatch = nn.ConvTranspose2d(embedding_dim, in_channel, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "        # QKV projection\n",
        "        self.qkv_proj = nn.Linear(embedding_dim, embedding_dim * 3)\n",
        "\n",
        "        # Output layer\n",
        "        self.output = nn.Conv2d(embedding_dim, embedding_dim, kernel_size=1)\n",
        "\n",
        "        # Positional embedding for patches\n",
        "        self.positional_encoding = nn.Parameter(positional_embedding(self.num_token, embedding_dim))\n",
        "        self.positional_encoding.requires_grad_(False)\n",
        "\n",
        "    def forward(self, tensor: torch.Tensor) -> torch.Tensor:\n",
        "        skip_tensor = tensor\n",
        "\n",
        "        tensor = self.gnorm1(tensor)\n",
        "\n",
        "        # Patch image\n",
        "        tensor = self.patch(tensor)\n",
        "        tensor = self.gnorm2(tensor)\n",
        "\n",
        "        # Reshape for self attention\n",
        "        batch_size, channel, height, width = tensor.shape\n",
        "\n",
        "        tensor = tensor.view(batch_size, channel, self.num_token)\n",
        "        tensor = tensor.permute(0, 2, 1)\n",
        "        # tensor = tensor + self.positional_encoding\n",
        "\n",
        "        tensor = self.qkv_proj(tensor)\n",
        "\n",
        "        query, key, value = torch.chunk(tensor, 3, dim=-1)\n",
        "        query = query.view(batch_size, self.num_token, self.num_head, self.head_dim)\n",
        "        key = key.view(batch_size, self.num_token, self.num_head, self.head_dim)\n",
        "        value = value.view(batch_size, self.num_token, self.num_head, self.head_dim)\n",
        "\n",
        "        query = query.transpose(1, 2)\n",
        "        key = key.transpose(1, 2)\n",
        "        value = value.transpose(1, 2)\n",
        "\n",
        "        # Self attention\n",
        "        attention_raw = torch.matmul(query, key.transpose(2, 3))\n",
        "        attention_scaled = attention_raw * self.scale\n",
        "        attention_score = torch.softmax(attention_scaled, dim=-1)\n",
        "        value = torch.matmul(attention_score, value)\n",
        "\n",
        "        # Reshape for self attention output\n",
        "        tensor = value.transpose(1, 2).contiguous()\n",
        "        tensor = tensor.view(batch_size, self.num_token, channel)\n",
        "        tensor = tensor.permute(0, 2, 1)\n",
        "        tensor = tensor.reshape(batch_size, channel, height, width)\n",
        "        tensor = self.output(tensor)\n",
        "\n",
        "        # Prepare for output\n",
        "        tensor = self.gnorm3(tensor)\n",
        "        tensor = self.unpatch(tensor)\n",
        "\n",
        "        tensor = tensor + skip_tensor\n",
        "\n",
        "        return tensor\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, image_channel: int = 3, image_size: int = 64, channels: list[int] = [32, 64, 128, 256], attention_channels = [256, 512], depth: int = 2, emb_dim: int = 1024, num_step: int = 1000, num_classes: int = 10, channel_per_group: int = 16, patch_size: int = 2, head_dim: int = 32):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.ModuleList([nn.ModuleList([nn.Conv2d(image_channel, channels[0], 3, padding=1)])])\n",
        "        self.decoder = nn.ModuleList()\n",
        "        skip_channel = [channels[0]]\n",
        "        image_size = [image_size // (2**i) for i in range(len(channels))]\n",
        "\n",
        "        self.positional_encoding = nn.Embedding(num_step, emb_dim)\n",
        "        self.positional_encoding.weight.data.copy_(positional_embedding(num_step, emb_dim))\n",
        "        self.positional_encoding.weight.requires_grad = False\n",
        "\n",
        "        # Encoder\n",
        "        for i in range(len(channels)):\n",
        "            for _ in range(depth):\n",
        "                layer = nn.ModuleList()\n",
        "                layer.append(ResBlock(channels[i], channels[i], emb_dim = emb_dim, channel_per_group=channel_per_group))\n",
        "                if channels[i] in attention_channels:\n",
        "                    layer.append(SelfAttentionBlock(channels[i], image_size[i]))\n",
        "                self.encoder.append(layer)\n",
        "                skip_channel.append(channels[i])\n",
        "\n",
        "            # Down projection\n",
        "            if i != len(channels)-1:\n",
        "                layer = nn.ModuleList()\n",
        "                layer.append(ResBlock(channels[i], channels[i + 1], down=True, emb_dim = emb_dim, channel_per_group=channel_per_group))\n",
        "                self.encoder.append(layer)\n",
        "                skip_channel.append(channels[i+1])\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottle_neck = nn.ModuleList([\n",
        "            ResBlock(channels[-1], channels[-1], channel_per_group=channel_per_group),\n",
        "            SelfAttentionBlock(channels[-1], image_size[-1]),\n",
        "            ResBlock(channels[-1], channels[-1], channel_per_group=channel_per_group),\n",
        "        ])\n",
        "\n",
        "        # Decoder\n",
        "        for i in range(len(channels)-1, -1, -1):\n",
        "            for _ in range(depth):\n",
        "                layer = nn.ModuleList()\n",
        "                layer.append(ResBlock(channels[i] + skip_channel.pop(), channels[i], emb_dim = emb_dim))\n",
        "                if channels[i] in attention_channels:\n",
        "                    layer.append(SelfAttentionBlock(channels[i], image_size[i]))\n",
        "                self.decoder.append(layer)\n",
        "\n",
        "            # Up projection\n",
        "            if i != 0:\n",
        "                layer = nn.ModuleList()\n",
        "                layer.append(ResBlock(channels[i] + skip_channel.pop(), channels[i - 1], up=True, emb_dim = emb_dim))\n",
        "                self.decoder.append(layer)\n",
        "            else:\n",
        "                layer = nn.ModuleList()\n",
        "                layer.append(ResBlock(channels[i] + skip_channel.pop(), channels[0], emb_dim = emb_dim))\n",
        "                self.decoder.append(layer)\n",
        "\n",
        "        self.time_embedding_proj = nn.Sequential(\n",
        "            nn.Linear(emb_dim, emb_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.class_embedding = nn.Embedding(num_classes, emb_dim)\n",
        "\n",
        "        # Output kernels to change back to image channel\n",
        "        self.out = nn.Sequential(\n",
        "            nn.GroupNorm(channels[0] // channel_per_group, channels[0], affine=False),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(channels[0], image_channel, kernel_size = 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, tensor: torch.Tensor, time_step: torch.Tensor, label: torch.Tensor = None) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Diffusion model.\n",
        "\n",
        "        :param tensor: Input tensor.\n",
        "        :param time_step: Time step tensor.\n",
        "        :param label: Label tensor.\n",
        "        :return: Predicted noise.\n",
        "        \"\"\"\n",
        "        embedding = self.positional_encoding(time_step)\n",
        "        embedding = self.time_embedding_proj(embedding)\n",
        "\n",
        "        if label != None:\n",
        "            class_embedding = self.class_embedding(label)\n",
        "            embedding = embedding + class_embedding\n",
        "\n",
        "        skip_connection = []\n",
        "\n",
        "        # Encoder\n",
        "        for layer in self.encoder:\n",
        "            for module in layer:\n",
        "                if(isinstance(module, ResBlock)):\n",
        "                    tensor = module(tensor, embedding)\n",
        "                else:\n",
        "                    tensor = module(tensor)\n",
        "\n",
        "            skip_connection.append(tensor)\n",
        "\n",
        "        # Bottleneck\n",
        "        for module in self.bottle_neck:\n",
        "            if(isinstance(module, ResBlock)):\n",
        "                tensor = module(tensor, embedding)\n",
        "            else:\n",
        "                tensor = module(tensor)\n",
        "\n",
        "        # Decoder\n",
        "        for layer in self.decoder:\n",
        "            tensor = torch.concatenate((tensor, skip_connection.pop()), dim = 1)\n",
        "            for module in layer:\n",
        "                if(isinstance(module, ResBlock)):\n",
        "                    tensor = module(tensor, embedding)\n",
        "                else:\n",
        "                    tensor = module(tensor)\n",
        "\n",
        "        tensor = self.out(tensor)\n",
        "\n",
        "        return tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GoEjKTQmnVjl"
      },
      "outputs": [],
      "source": [
        "# Define the transformation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(125),\n",
        "    transforms.RandomResizedCrop(image_size, scale=(0.8, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load the training set\n",
        "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "# Load the test set\n",
        "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Create a DataLoader for the combined dataset\n",
        "train_dataloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "valid_dataloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XB5rRMonmK0"
      },
      "outputs": [],
      "source": [
        "# Instantiate the model\n",
        "unet = UNet(num_classes=num_class).to(device)\n",
        "print(\"This model has\", sum(p.numel() for p in unet.parameters()), \"parameters.\")\n",
        "scaler = amp.GradScaler()\n",
        "loss_train = []\n",
        "loss_valid = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTafC6Lennp3"
      },
      "outputs": [],
      "source": [
        "# Set up optimizer and loss\n",
        "optimizer = opt.AdamW(unet.parameters(), lr = lr, weight_decay = weight_decay)\n",
        "criterion = nn.MSELoss()\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIUbXErUnrOA"
      },
      "outputs": [],
      "source": [
        "for epoch in range(epochs):\n",
        "    train_loss_list = []\n",
        "    valid_loss_list = []\n",
        "    for images, label in tqdm(train_dataloader):\n",
        "        # Zero out grad\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Preparing for forward pass\n",
        "        images = images.to(device)\n",
        "        label = label.to(device)\n",
        "        time_step = torch.randint(1, steps, size = (images.shape[0], )).to(device)\n",
        "        x_t, noise = forward_pass(images, time_step)\n",
        "\n",
        "        # Classifier free guidance.\n",
        "        if random.random() < 0.1:\n",
        "            label = None\n",
        "\n",
        "        # Forward pass\n",
        "        with amp.autocast():\n",
        "            predicted_noise = unet(x_t, time_step, label)\n",
        "            loss = criterion(predicted_noise, noise)\n",
        "\n",
        "        # Back propagation\n",
        "        scaler.scale(loss).backward()\n",
        "        torch.nn.utils.clip_grad_norm_(unet.parameters(), max_norm=1.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # Record loss\n",
        "        train_loss_list.append(loss.item())\n",
        "\n",
        "    if(epoch % 10 == 0):\n",
        "        with torch.no_grad():\n",
        "            for images, label in tqdm(valid_dataloader):\n",
        "                # Preparing for forward pass\n",
        "                images = images.to(device)\n",
        "                label = label.to(device)\n",
        "                time_step = torch.randint(1, steps, size = (images.shape[0], )).to(device)\n",
        "                x_t, noise = forward_pass(images, time_step)\n",
        "\n",
        "                # Forward pass\n",
        "                with amp.autocast():\n",
        "                    predicted_noise = unet(x_t, time_step, label)\n",
        "                    loss = criterion(predicted_noise, noise)\n",
        "                valid_loss_list.append(loss.item())\n",
        "\n",
        "    # Step the learning rate\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f\"Epoch #{epoch}\")\n",
        "    print(f\"Current learning rate is {optimizer.param_groups[0]['lr']}\")\n",
        "    print(\"Train Loss is:\", sum(train_loss_list)/len(train_loss_list))\n",
        "    loss_train.append(sum(train_loss_list)/len(train_loss_list))\n",
        "    if(epoch % 10 == 0):\n",
        "        print(\"Valid Loss is:\", sum(valid_loss_list)/len(valid_loss_list))\n",
        "        loss_valid.append(sum(valid_loss_list)/len(valid_loss_list))\n",
        "    if(epoch % 10 == 0):\n",
        "        label = torch.tensor([0, 1]).to(device)\n",
        "        sampling(unet, label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhi1Rx1Y1GE5"
      },
      "outputs": [],
      "source": [
        "torch.save(unet, \"diffusion_CIFAR10.pth\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "09a84bd1dc98317f17d8982ec5713d832b330e2f9a95d130d3bd5924dbe90d6c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}