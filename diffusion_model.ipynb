{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c357bbcd-7676-4f55-aaa7-19e3b7811898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as opt\n",
    "from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "# Reproducibility\n",
    "import random\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "375ac356-8d0f-4427-9c36-6dc2ff20124c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "device = \"mps\"\n",
    "weight_decay = 0.01\n",
    "lr = 3e-4\n",
    "batch_size = 60\n",
    "epochs = 1000\n",
    "dataset_filepath = \"./ImagenetHighResolution\"\n",
    "image_size = 32\n",
    "num_steps = 1000\n",
    "channel_mean = [0.4918695390224457, 0.4826536178588867, 0.44717657566070557] # CIFAR10\n",
    "channel_std = [0.20224887132644653, 0.19936397671699524, 0.2009899914264679] # CIFAR10\n",
    "# channel_mean = [0.4753786623477936, 0.4518871307373047, 0.40141433477401733] # ImageNet\n",
    "# channel_std = [0.22465701401233673, 0.22012709081172943, 0.22106702625751495] # ImageNet\n",
    "num_class = 10\n",
    "channel_num = 128\n",
    "dropout_ratio = 0.1\n",
    "block_per_resolution = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36e85e0a-4f8c-4023-a1c1-81bf2aed1097",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaNorm(nn.Module):\n",
    "    def __init__(self, channel_num: int, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        self.normalization = nn.BatchNorm2d(channel_num, affine=False)\n",
    "        self.scale_shift = nn.Linear(embedding_dim, channel_num*2)\n",
    "        self.activation = nn.SiLU()\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor, embedding: torch.Tensor) -> torch.Tensor:\n",
    "        tensor = self.normalization(tensor)\n",
    "        embedding = self.activation(embedding)\n",
    "        scale_shift_tensor = self.scale_shift(embedding)\n",
    "        scale_shift_tensor = scale_shift_tensor[..., None, None]\n",
    "        scale, shift = torch.chunk(scale_shift_tensor, 2, dim=1)\n",
    "        tensor = tensor * (1 + scale) + shift\n",
    "        return tensor\n",
    "\n",
    "class BigGANsBlock(nn.Module):\n",
    "    def __init__(self, in_channel: int, embedding_dim: int, dropout: float = 0.5, out_channel: int | None = None, up: bool = False, down: bool = False):\n",
    "        super().__init__()\n",
    "        # Init value of out_channel\n",
    "        if not out_channel:\n",
    "            out_channel = in_channel\n",
    "        \n",
    "        if down:\n",
    "            self.down_sample = nn.MaxPool2d(2)\n",
    "\n",
    "        if in_channel == out_channel:\n",
    "            self.skip_connection = nn.Identity()\n",
    "        else:\n",
    "            self.skip_connection = nn.Conv2d(in_channel, out_channel, 1)\n",
    "\n",
    "        self.batch_norm = nn.BatchNorm2d(in_channel, affine=False)\n",
    "        self.activation = nn.SiLU()\n",
    "        self.conv1 = nn.Conv2d(in_channel, out_channel, 3, padding=1)\n",
    "        self.ada_norm = AdaNorm(out_channel, embedding_dim)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.conv2 = nn.Conv2d(out_channel, out_channel, 3, padding=1)\n",
    "        self.up = up\n",
    "        self.down = down\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor, embedding: torch.Tensor) -> torch.Tensor:\n",
    "        # Skip connection\n",
    "        skip_tensor = tensor\n",
    "        if self.up:\n",
    "            skip_tensor = F.interpolate(skip_tensor, scale_factor=2, mode=\"nearest\")\n",
    "        elif self.down:\n",
    "            skip_tensor = self.down_sample(skip_tensor)\n",
    "        skip_tensor = self.skip_connection(skip_tensor)\n",
    "\n",
    "        # Main connection\n",
    "        tensor = self.batch_norm(tensor)\n",
    "        tensor = self.activation(tensor)\n",
    "        if self.up:\n",
    "            tensor = F.interpolate(tensor, scale_factor=2, mode=\"nearest\")\n",
    "        elif self.down:\n",
    "            tensor = self.down_sample(tensor)\n",
    "        tensor = self.conv1(tensor)\n",
    "\n",
    "        tensor = self.ada_norm(tensor, embedding)\n",
    "        tensor = self.activation(tensor)\n",
    "        tensor = self.dropout(tensor)\n",
    "        tensor = self.conv2(tensor)\n",
    "\n",
    "        return tensor + skip_tensor\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, channel_num: int, channel_per_head: int = 64, dropout_ratio: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.qkv = nn.Linear(channel_num, channel_num*3)\n",
    "        self.o = nn.Linear(channel_num, channel_num)\n",
    "        self.scaler = 1/math.sqrt(channel_per_head)\n",
    "        self.attn_dropout = nn.Dropout(dropout_ratio)\n",
    "        self.norm = nn.BatchNorm2d(channel_num, affine=False)\n",
    "\n",
    "        assert channel_num % channel_per_head == 0\n",
    "        self.num_head = int(channel_num / channel_per_head)\n",
    "        self.head_dim = channel_per_head\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        # Skip connection\n",
    "        skip_connection = tensor\n",
    "\n",
    "        # Record shapes\n",
    "        batch_size, channel_num, height, width = tensor.shape\n",
    "        num_pixel: int = height * width\n",
    "\n",
    "        # Classic self attention without attention mask\n",
    "        tensor = self.norm(tensor)\n",
    "        tensor = tensor.reshape(batch_size, channel_num, num_pixel).permute(0, 2, 1).contiguous()\n",
    "        qkv_tensor = self.qkv(tensor)\n",
    "        query, key, value = qkv_tensor.split([channel_num, channel_num, channel_num], dim=-1)\n",
    "\n",
    "        query = query.contiguous().reshape(batch_size, num_pixel, self.num_head, self.head_dim)\n",
    "        key = key.contiguous().reshape(batch_size, num_pixel, self.num_head, self.head_dim)\n",
    "        value = value.contiguous().reshape(batch_size, num_pixel, self.num_head, self.head_dim)\n",
    "\n",
    "        # Switch to batch_size, self.num_head, num_pixel, self.head_dim\n",
    "        query = query.transpose(1, 2)\n",
    "        key = key.transpose(1, 2)\n",
    "        value = value.transpose(1, 2)\n",
    "\n",
    "        attention_raw = torch.matmul(query, key.transpose(2, 3))\n",
    "        attention_scaled = attention_raw * self.scaler\n",
    "        attention_score = torch.softmax(attention_scaled, dim=-1)\n",
    "        attention_score = self.attn_dropout(attention_score)\n",
    "        value = torch.matmul(attention_score, value)\n",
    "\n",
    "        # Reshape back to batch_size, num_pixel, channel_num\n",
    "        value = value.transpose(1, 2).contiguous()\n",
    "        value = value.reshape(batch_size, num_pixel, channel_num)\n",
    "\n",
    "        # Output layer\n",
    "        output = self.o(value)\n",
    "        output = self.attn_dropout(output)\n",
    "\n",
    "        # Reshape back to batch_size, channel_num, height, width\n",
    "        output = output.permute(0, 2, 1).contiguous().reshape(batch_size, channel_num, height, width)\n",
    "        output = output + skip_connection\n",
    "        return output\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, image_channel: int = 3, image_size: int = 64, channel_num: int = 64, embedding_dim: int = 256, channel_per_head: int = 64, dropout_ratio: float = 0.5, block_per_resolution: int = 2, num_step: int = 1000, num_class: int = 1000):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.ModuleList([nn.ModuleList([nn.Conv2d(image_channel, channel_num, 3, padding=1)])])\n",
    "        self.decoder = nn.ModuleList()\n",
    "\n",
    "        # Create config lists\n",
    "        attention_resolution: list[int] = [32, 16, 8]\n",
    "        resolution_list: list[int] = []\n",
    "        channel_list: list[int] = []\n",
    "        \n",
    "        current_image_size: int = image_size\n",
    "        current_channel_num: int = channel_num\n",
    "        counter = 1\n",
    "        while current_image_size >= 8:\n",
    "            resolution_list.append(current_image_size)\n",
    "            channel_list.append(channel_num * counter)\n",
    "            current_image_size = int(current_image_size / 2)\n",
    "            counter += 1\n",
    "        skip_channel = [channel_list[0]]\n",
    "\n",
    "        # Create positional embedding\n",
    "        self.positional_embedding = nn.Parameter(torch.randn(num_step, embedding_dim) / embedding_dim ** 0.5)\n",
    "        self.class_embedding = nn.Parameter(torch.randn(num_class, embedding_dim) / embedding_dim ** 0.5)\n",
    "\n",
    "        # Encoder\n",
    "        for i in range(len(channel_list)):\n",
    "            for _ in range(block_per_resolution):\n",
    "                layer = nn.ModuleList()\n",
    "                layer.append(BigGANsBlock(channel_list[i], embedding_dim))\n",
    "                if resolution_list[i] in attention_resolution:\n",
    "                    layer.append(Attention(channel_list[i]))\n",
    "                self.encoder.append(layer)\n",
    "                skip_channel.append(channel_list[i])\n",
    "\n",
    "            # Down projection\n",
    "            if i != len(channel_list)-1:\n",
    "                layer = nn.ModuleList()\n",
    "                layer.append(BigGANsBlock(channel_list[i], embedding_dim, out_channel=channel_list[i + 1], down=True))\n",
    "                self.encoder.append(layer)\n",
    "                skip_channel.append(channel_list[i+1])\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottle_neck = nn.ModuleList([\n",
    "            BigGANsBlock(channel_list[-1], embedding_dim),\n",
    "            Attention(channel_list[-1]),\n",
    "            BigGANsBlock(channel_list[-1], embedding_dim),\n",
    "        ])\n",
    "\n",
    "        # Decoder\n",
    "        for i in range(len(channel_list)-1, -1, -1):\n",
    "            for _ in range(block_per_resolution):\n",
    "                layer = nn.ModuleList()\n",
    "                layer.append(BigGANsBlock(channel_list[i] + skip_channel.pop(), embedding_dim, out_channel=channel_list[i]))\n",
    "                if resolution_list[i] in attention_resolution:\n",
    "                    layer.append(Attention(channel_list[i]))\n",
    "                self.decoder.append(layer)\n",
    "\n",
    "            # Up projection\n",
    "            if i != 0:\n",
    "                layer = nn.ModuleList()\n",
    "                layer.append(BigGANsBlock(channel_list[i] + skip_channel.pop(), embedding_dim, out_channel=channel_list[i - 1], up=True))\n",
    "                self.decoder.append(layer)\n",
    "            else:\n",
    "                layer = nn.ModuleList()\n",
    "                layer.append(BigGANsBlock(channel_list[i] + skip_channel.pop(), embedding_dim, out_channel=channel_list[0]))\n",
    "                self.decoder.append(layer)\n",
    "\n",
    "        # Output kernels to change back to image channel\n",
    "        self.out = nn.Sequential(\n",
    "            nn.BatchNorm2d(channel_list[0], affine=False),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(channel_list[0], image_channel, kernel_size = 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor, time_step: torch.Tensor, label: torch.Tensor = None) -> torch.Tensor:\n",
    "        embedding = self.positional_embedding[time_step]\n",
    "\n",
    "        if label != None:\n",
    "            label_embedding = self.class_embedding[label]\n",
    "            embedding = embedding + label_embedding\n",
    "\n",
    "        skip_connection = []\n",
    "\n",
    "        # Encoder\n",
    "        for layer in self.encoder:\n",
    "            for module in layer:\n",
    "                if(isinstance(module, BigGANsBlock)):\n",
    "                    tensor = module(tensor, embedding)\n",
    "                else:\n",
    "                    tensor = module(tensor)\n",
    "\n",
    "            skip_connection.append(tensor)\n",
    "\n",
    "        # Bottleneck\n",
    "        for module in self.bottle_neck:\n",
    "            if(isinstance(module, BigGANsBlock)):\n",
    "                tensor = module(tensor, embedding)\n",
    "            else:\n",
    "                tensor = module(tensor)\n",
    "\n",
    "        # Decoder\n",
    "        for layer in self.decoder:\n",
    "            tensor = torch.concatenate((tensor, skip_connection.pop()), dim = 1)\n",
    "            for module in layer:\n",
    "                if(isinstance(module, BigGANsBlock)):\n",
    "                    tensor = module(tensor, embedding)\n",
    "                else:\n",
    "                    tensor = module(tensor)\n",
    "\n",
    "        tensor = self.out(tensor)\n",
    "\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f92be09-a3dd-4371-8654-0e475ec65b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image transformations for preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=channel_mean, std=channel_std)\n",
    "])\n",
    "\n",
    "def valid_image_folder(path: str) -> bool:\n",
    "    # Check if file starts with '._' or ends with '.DS_Store'\n",
    "    filename = os.path.basename(path)\n",
    "    if filename.startswith(\"._\") or filename == \".DS_Store\": # Stupid MacOS\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Use ImageFolder to automatically label images based on folder names\n",
    "dataset = datasets.ImageFolder(root=dataset_filepath, is_valid_file=valid_image_folder, transform=transform)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "g = torch.Generator().manual_seed(seed)\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_size, val_size, test_size], generator=g\n",
    ")\n",
    "\n",
    "# Create DataLoaders for training and validation\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c313f283-8c6a-4ccc-ab92-1a8ec1136bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image transformations for preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=channel_mean, std=channel_std)\n",
    "])\n",
    "\n",
    "# Load the training set\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Load the test set\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create a DataLoader for the combined dataset\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c6b4342-89f1-4f76-85a8-5b531b95a6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model has 41046019 parameters.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "unet = UNet(image_size=image_size, num_class=num_class, channel_num=channel_num, dropout_ratio=dropout_ratio, block_per_resolution=block_per_resolution).to(device)\n",
    "print(\"This model has\", sum(p.numel() for p in unet.parameters()), \"parameters.\")\n",
    "loss_train = []\n",
    "loss_valid = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d0fea03-cb77-4645-af4b-6f4d61cb86e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# util.py\n",
    "beta_min = 1e-4\n",
    "beta_max = 0.02\n",
    "\n",
    "# Constants required for forward process\n",
    "beta = torch.linspace(beta_min, beta_max, num_steps)\n",
    "alpha = 1 - beta\n",
    "alphas_cumprod = torch.cumprod(alpha, axis=0)\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod).reshape(-1, 1, 1, 1).to(device)\n",
    "one_minus_alphas_cumprod = 1 - alphas_cumprod\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(one_minus_alphas_cumprod).reshape(-1, 1, 1, 1).to(device)\n",
    "\n",
    "# Constants required for backward process\n",
    "one_over_sqrt_alpha = 1/torch.sqrt(alpha).view(-1, 1, 1, 1).to(device)\n",
    "one_minus_alpha = (1 - alpha).view(-1, 1, 1, 1).to(device)\n",
    "sqrt_beta = torch.sqrt(beta).view(-1, 1, 1, 1).to(device)\n",
    "\n",
    "def forward_process(images: torch.Tensor, timesteps: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    batched_sqrt_alphas_cumprod = sqrt_alphas_cumprod[timesteps]\n",
    "    batched_sqrt_one_minus_alphas_cumprod = sqrt_one_minus_alphas_cumprod[timesteps]\n",
    "    noise = torch.randn_like(images)\n",
    "\n",
    "    return batched_sqrt_alphas_cumprod * images + batched_sqrt_one_minus_alphas_cumprod * noise, noise\n",
    "\n",
    "def sampling(model: UNet, labels: torch.Tensor, cfg_scale: int = 3) -> None:    \n",
    "    with torch.no_grad():\n",
    "        x = torch.randn(labels.shape[0], 3, image_size, image_size).to(device)\n",
    "\n",
    "        for i in tqdm(range(num_steps-1, -1, -1)):\n",
    "            t = torch.tensor([i]*labels.shape[0]).to(device)\n",
    "\n",
    "            # Classifier free guidance\n",
    "            predicted_noise_no_label = model(x, t, None)\n",
    "            predicted_noise_with_label = model(x, t, labels)\n",
    "            predicted_noise = torch.lerp(predicted_noise_no_label, predicted_noise_with_label, cfg_scale)\n",
    "\n",
    "            # Sample for input to next step\n",
    "            if(i == 0):\n",
    "                noise = torch.zeros_like(x).to(device)\n",
    "            else:\n",
    "                noise = torch.randn_like(x).to(device)\n",
    "            x = one_over_sqrt_alpha[t] * (x - ((one_minus_alpha[t])/(sqrt_one_minus_alphas_cumprod[t]))*predicted_noise) + sqrt_beta[t] * noise\n",
    "\n",
    "        # Turn back into [0, 1] range\n",
    "        mean = torch.tensor(channel_mean, device=device).view(1, 3, 1, 1)\n",
    "        std = torch.tensor(channel_std, device=device).view(1, 3, 1, 1)\n",
    "        \n",
    "        x = x * std + mean\n",
    "        x = x.clamp(0, 1)\n",
    "\n",
    "        # Visualize it\n",
    "        x = x.cpu().permute(0, 2, 3, 1).numpy()\n",
    "        for i in range(x.shape[0]):\n",
    "            plt.imshow(x[i])\n",
    "            plt.axis('off')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9c4e0a6-b9b9-4347-b349-87a7d5c6abc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up optimizer and loss\n",
    "optimizer = opt.AdamW(unet.parameters(), lr = lr, weight_decay = weight_decay)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eaac086b-e90b-4afa-8b55-226229cbd92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps = T_max=epochs*len(train_loader)\n",
    "warmup_steps = int(total_steps * 0.05)\n",
    "\n",
    "# Warmup: LR linearly increases from 0 → base LR over warmup_steps\n",
    "warmup_scheduler = LinearLR(optimizer, start_factor=0.01, end_factor=1.0, total_iters=warmup_steps)\n",
    "\n",
    "# Cosine annealing: after warmup\n",
    "cosine_scheduler = CosineAnnealingLR(optimizer, T_max=(total_steps - warmup_steps), eta_min=1e-5)\n",
    "\n",
    "# Combine them\n",
    "scheduler = SequentialLR(optimizer, schedulers=[warmup_scheduler, cosine_scheduler], milestones=[warmup_steps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76ae5960-fc6c-4c90-a156-0000231b9973",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|███████████████████▊                     | 404/834 [07:27<07:56,  1.11s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m loss = criterion(predicted_noise, noise)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Back propagation\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m torch.nn.utils.clip_grad_norm_(unet.parameters(), max_norm=\u001b[32m1.0\u001b[39m)\n\u001b[32m     26\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/ViT/.venv/lib/python3.13/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/ViT/.venv/lib/python3.13/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/ViT/.venv/lib/python3.13/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    train_loss_list = []\n",
    "    valid_loss_list = []\n",
    "    unet.train()\n",
    "    for images, label in tqdm(train_loader):\n",
    "        # Zero out grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Preparing for forward pass\n",
    "        images = images.to(device)\n",
    "        label = label.to(device)\n",
    "        time_step = torch.randint(1, num_steps, size = (images.shape[0], )).to(device)\n",
    "        x_t, noise = forward_process(images, time_step)\n",
    "\n",
    "        # Classifier free guidance.\n",
    "        if random.random() < 0.1:\n",
    "            label = None\n",
    "\n",
    "        # Forward pass\n",
    "        predicted_noise = unet(x_t, time_step, label)\n",
    "        loss = criterion(predicted_noise, noise)\n",
    "\n",
    "        # Back propagation\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(unet.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Record loss\n",
    "        train_loss_list.append(loss.item())\n",
    "\n",
    "        # Step the learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "    unet.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, label in tqdm(val_loader):\n",
    "            # Preparing for forward pass\n",
    "            images = images.to(device)\n",
    "            label = label.to(device)\n",
    "            time_step = torch.randint(1, num_steps, size = (images.shape[0], )).to(device)\n",
    "            x_t, noise = forward_process(images, time_step)\n",
    "\n",
    "            # Forward pass\n",
    "            predicted_noise = unet(x_t, time_step, label)\n",
    "            loss = criterion(predicted_noise, noise)\n",
    "            valid_loss_list.append(loss.item())\n",
    "\n",
    "    print(f\"Epoch #{epoch}\")\n",
    "    print(f\"Current learning rate is {optimizer.param_groups[0]['lr']}\")\n",
    "    print(\"Train Loss is:\", sum(train_loss_list)/len(train_loss_list))\n",
    "    loss_train.append(sum(train_loss_list)/len(train_loss_list))\n",
    "    print(\"Valid Loss is:\", sum(valid_loss_list)/len(valid_loss_list))\n",
    "    loss_valid.append(sum(valid_loss_list)/len(valid_loss_list))\n",
    "    if epoch % 100 == 0:\n",
    "        label = torch.tensor([0, 1]).to(device)\n",
    "        sampling(unet, label)\n",
    "    torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bfdcf8-1ff1-478c-9a22-3b962da598cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(unet, \"batch_64_1000_epoch_CIFAR10.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
