{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wki9cpQZ7dxN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x79a3e45780f0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as opt\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms, datasets\n",
    "from torch.cuda import amp\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "C5fmBN1VmwjY"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "beta_start = 1e-4\n",
    "beta_end = 0.02\n",
    "steps = 1000\n",
    "device_id = 0\n",
    "image_size = 64\n",
    "image_channel = 3\n",
    "epochs = 300\n",
    "lr = 3e-4\n",
    "weight_decay = 0\n",
    "batch_size = 1\n",
    "num_class = 1000\n",
    "pos_dim = 1024\n",
    "dataset_filepath = \"/media/danjie_tang/Danjie HDD/Imagenet/ILSVRC/Data/CLS-LOC/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "IehB5lv6hgvq"
   },
   "outputs": [],
   "source": [
    "# Constants used for diffusion model\n",
    "beta = torch.linspace(beta_start, beta_end, steps).cuda(device_id)\n",
    "sqrt_beta = torch.sqrt(beta).view(-1, 1, 1, 1)\n",
    "alpha = 1 - beta\n",
    "alphas_cumprod = torch.cumprod(alpha, axis=0)\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod).view(-1, 1, 1, 1)\n",
    "one_minus_alphas_cumprod = 1 - alphas_cumprod\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(one_minus_alphas_cumprod).view(-1, 1, 1, 1)\n",
    "one_over_sqrt_alpha = 1/torch.sqrt(alpha).view(-1, 1, 1, 1)\n",
    "one_minus_alpha = (1 - alpha).view(-1, 1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "sG4RT6IBnDy6"
   },
   "outputs": [],
   "source": [
    "# util.py\n",
    "\n",
    "# Forward pass\n",
    "def forward_pass(images, t):\n",
    "    batch_sqrt_alphas_cumprod = sqrt_alphas_cumprod[t]\n",
    "    batch_sqrt_one_minus_alphas_cumprod = sqrt_one_minus_alphas_cumprod[t]\n",
    "    noise = torch.randn_like(images).cuda(device_id)\n",
    "\n",
    "    return batch_sqrt_alphas_cumprod * images + batch_sqrt_one_minus_alphas_cumprod * noise, noise\n",
    "\n",
    "# Sampling\n",
    "def sampling(model, labels, cfg_scale: int = 3):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x = torch.randn(labels.shape[0], image_channel, image_size, image_size).cuda(device_id)\n",
    "\n",
    "        for i in tqdm(range(steps-1, -1, -1)):\n",
    "            t = torch.tensor([i]*labels.shape[0]).cuda(device_id)\n",
    "\n",
    "            # Classifier free guidance\n",
    "            predicted_noise_no_label = model(x, t, None)\n",
    "            predicted_noise_with_label = model(x, t, labels)\n",
    "            predicted_noise = torch.lerp(predicted_noise_no_label, predicted_noise_with_label, cfg_scale)\n",
    "\n",
    "            if(i == 0):\n",
    "                noise = torch.zeros_like(x).cuda(device_id)\n",
    "            else:\n",
    "                noise = torch.randn_like(x).cuda(device_id)\n",
    "\n",
    "            x = one_over_sqrt_alpha[t] * (x - ((one_minus_alpha[t])/(sqrt_one_minus_alphas_cumprod[t]))*predicted_noise) + sqrt_beta[t] * noise\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    x = (x.clamp(-1, 1) + 1) / 2\n",
    "    x = (x * 255).type(torch.uint8)\n",
    "\n",
    "    for i in range(x.shape[0]):\n",
    "        tensor = x[i].permute(1, 2, 0).to(\"cpu\")\n",
    "        plt.imshow(tensor)\n",
    "        plt.show()\n",
    "\n",
    "def positional_embedding(num_step: int, emb_dim: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create positional embedding tensor.\n",
    "\n",
    "    :param num_step: Number of time steps.\n",
    "    :param emb_dim: Embedding dimension.\n",
    "    :return: Positional embedding tensor.\n",
    "    \"\"\"\n",
    "    matrix = torch.zeros(num_step, emb_dim)\n",
    "    for i in range(num_step):\n",
    "        for j in range(0, emb_dim, 2):\n",
    "            matrix[i, j] = np.sin(i/(10000**(j/emb_dim)))\n",
    "            if(j+1<emb_dim):\n",
    "                matrix[i, j+1] = np.cos(i/(10000**(j/emb_dim)))\n",
    "\n",
    "    return matrix\n",
    "\n",
    "def sinusoidal_positional_encoding_2d(height: int, width: int, channel: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generate a 2D sinusoidal positional encoding.\n",
    "\n",
    "    :param height: The height of the encoding.\n",
    "    :param width: The width of the encoding.\n",
    "    :param channel: The number of channels in the encoding.\n",
    "    :return: A tensor of shape (height, width, channel) containing the 2D positional encoding.\n",
    "    \"\"\"\n",
    "    if channel % 2 != 0:\n",
    "        raise ValueError(\"The 'channel' dimension must be an even number.\")\n",
    "\n",
    "    # First, build in (height, width, channel) format\n",
    "    pe = torch.zeros(height, width, channel)\n",
    "\n",
    "    half_ch = channel // 2\n",
    "\n",
    "    # Precompute the exponent for row and column\n",
    "    row_div_term = torch.exp(\n",
    "        -math.log(10000.0) * (torch.arange(0, half_ch, 2).float() / half_ch)\n",
    "    )\n",
    "    col_div_term = torch.exp(\n",
    "        -math.log(10000.0) * (torch.arange(0, half_ch, 2).float() / half_ch)\n",
    "    )\n",
    "\n",
    "    for h in range(height):\n",
    "        for w in range(width):\n",
    "            # Encode row index (h) into the first half of the channels\n",
    "            for i in range(0, half_ch, 2):\n",
    "                pe[h, w, i]     = math.sin(h * row_div_term[i // 2])\n",
    "                pe[h, w, i + 1] = math.cos(h * row_div_term[i // 2])\n",
    "\n",
    "            # Encode column index (w) into the second half of the channels\n",
    "            for j in range(0, half_ch, 2):\n",
    "                pe[h, w, half_ch + j]     = math.sin(w * col_div_term[j // 2])\n",
    "                pe[h, w, half_ch + j + 1] = math.cos(w * col_div_term[j // 2])\n",
    "\n",
    "    # Permute to get the shape (channel, width, height).\n",
    "    # Currently pe is (height, width, channel) = (H, W, C)\n",
    "    # We want (C, W, H), so we do permute(2, 1, 0).\n",
    "    pe = pe.permute(2, 1, 0)  # => (channel, width, height)\n",
    "\n",
    "    return pe\n",
    "\n",
    "def zero_out(layer):\n",
    "    for p in layer.parameters():\n",
    "        p.detach().zero_()\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "hT8TE3hV7rZl"
   },
   "outputs": [],
   "source": [
    "# model.py\n",
    "\n",
    "class AdaNorm(nn.Module):\n",
    "    def __init__(self, num_channel: int, channel_per_group: int = 16, emb_dim: int = 1024):\n",
    "        super().__init__()\n",
    "        assert num_channel % channel_per_group == 0, \"num_channel must be divisible by channel_per_group\"\n",
    "        num_group = num_channel // channel_per_group\n",
    "        self.embedding_proj = nn.Sequential(\n",
    "            nn.Linear(emb_dim, 2 * num_channel),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.gnorm = nn.GroupNorm(num_group, num_channel, affine=False)\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor, embedding: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform adanormalization on input tensor.\n",
    "\n",
    "        :param tensor: Input tensor to be normalized.\n",
    "        :param embedding: Embedding tensor containing time embedding and potentially class embedding.\n",
    "        :return: Normalized tensor.\n",
    "        \"\"\"\n",
    "        embedding = self.embedding_proj(embedding)\n",
    "        embedding = embedding.view(embedding.shape[0], embedding.shape[1], 1, 1)\n",
    "        scale, shift = torch.chunk(embedding, 2, dim=1)\n",
    "\n",
    "        tensor = self.gnorm(tensor)\n",
    "        # tensor = tensor * torch.sigmoid(scale) + shift\n",
    "        tensor = tensor * (1 + scale) + shift\n",
    "        return tensor\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channel: int, out_channel: int, emb_dim: int = 1024, up: bool = False, down: bool = False, channel_per_group: int = 16):\n",
    "        super().__init__()\n",
    "\n",
    "        # Upsampling or downsampling only for skip connection\n",
    "        self.up = up\n",
    "        self.down = down\n",
    "\n",
    "        # Normalization layers\n",
    "        self.norm1 = AdaNorm(in_channel, emb_dim=emb_dim)\n",
    "        self.norm2 = AdaNorm(out_channel, emb_dim=emb_dim)\n",
    "\n",
    "        # Convolution layers\n",
    "        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=3, padding=1)\n",
    "        self.conv2 = zero_out(nn.Conv2d(out_channel, out_channel, kernel_size=3, padding=1))\n",
    "\n",
    "        # Skip connection\n",
    "        if in_channel != out_channel or up or down:\n",
    "            self.skip_connection = nn.Sequential(\n",
    "                nn.Conv2d(in_channel, out_channel, kernel_size=1),\n",
    "                nn.Upsample(scale_factor=2) if up else nn.Identity(),\n",
    "                nn.AvgPool2d(kernel_size=2) if down else nn.Identity(),\n",
    "            )\n",
    "        else:\n",
    "            self.skip_connection = nn.Identity()\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor, embedding: torch.Tensor) -> torch.Tensor:\n",
    "        skip_tensor = self.skip_connection(tensor)\n",
    "\n",
    "        # Main path\n",
    "        tensor = self.norm1(tensor, embedding)\n",
    "        tensor = F.relu(tensor)\n",
    "        if self.up:\n",
    "            tensor = F.interpolate(tensor, scale_factor=2)\n",
    "        if self.down:\n",
    "            tensor = F.avg_pool2d(tensor, kernel_size=2)\n",
    "        tensor = self.conv1(tensor)\n",
    "        tensor = self.norm2(tensor, embedding)\n",
    "        tensor = F.relu(tensor)\n",
    "        tensor = self.conv2(tensor)\n",
    "\n",
    "        tensor += skip_tensor\n",
    "        return tensor\n",
    "\n",
    "\n",
    "class SelfAttentionBlock(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, image_size: int, head_dim: int = 64, channel_per_group: int = 16):\n",
    "        super().__init__()\n",
    "        self.head_dim: int = head_dim\n",
    "        self.num_head: int = embedding_dim // head_dim\n",
    "        self.scale: float = head_dim ** -0.5\n",
    "        self.num_pixel = image_size ** 2\n",
    "        self.gnorm1 = nn.GroupNorm(embedding_dim // channel_per_group, embedding_dim)\n",
    "        self.gnorm2 = nn.GroupNorm(embedding_dim // channel_per_group, embedding_dim)\n",
    "\n",
    "        # QKV projection\n",
    "        self.qkv_proj = nn.Linear(embedding_dim, embedding_dim * 3)\n",
    "\n",
    "        # Output layer\n",
    "        self.output = zero_out(nn.Conv2d(embedding_dim, embedding_dim, kernel_size=1))\n",
    "\n",
    "        # Positional embedding for patches\n",
    "        self.positional_encoding = nn.Parameter(sinusoidal_positional_encoding_2d(image_size, image_size, embedding_dim))\n",
    "        self.positional_encoding.requires_grad_(False)\n",
    "\n",
    "        # Feed Forward Layer\n",
    "        self.ffn1 = nn.Conv2d(embedding_dim, embedding_dim * 8, kernel_size=1)\n",
    "        self.ffn2 = nn.Conv2d(embedding_dim * 8, embedding_dim, kernel_size=1)\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        skip_tensor = tensor\n",
    "\n",
    "        tensor = self.gnorm1(tensor)\n",
    "\n",
    "        # Reshape for self attention\n",
    "        batch_size, channel, height, width = tensor.shape\n",
    "        tensor = tensor + self.positional_encoding\n",
    "        tensor = tensor.view(batch_size, channel, self.num_pixel)\n",
    "        tensor = tensor.permute(0, 2, 1)\n",
    "\n",
    "        tensor = self.qkv_proj(tensor)\n",
    "\n",
    "        query, key, value = torch.chunk(tensor, 3, dim=-1)\n",
    "        query = query.view(batch_size, self.num_pixel, self.num_head, self.head_dim)\n",
    "        key = key.view(batch_size, self.num_pixel, self.num_head, self.head_dim)\n",
    "        value = value.view(batch_size, self.num_pixel, self.num_head, self.head_dim)\n",
    "\n",
    "        query = query.transpose(1, 2)\n",
    "        key = key.transpose(1, 2)\n",
    "        value = value.transpose(1, 2)\n",
    "\n",
    "        # Self attention\n",
    "        attention_raw = torch.matmul(query, key.transpose(2, 3))\n",
    "        attention_scaled = attention_raw * self.scale\n",
    "        attention_score = torch.softmax(attention_scaled, dim=-1)\n",
    "        value = torch.matmul(attention_score, value)\n",
    "\n",
    "        # Reshape for self attention output\n",
    "        tensor = value.transpose(1, 2).contiguous()\n",
    "        tensor = tensor.view(batch_size, self.num_pixel, channel)\n",
    "        tensor = tensor.permute(0, 2, 1)\n",
    "        tensor = tensor.reshape(batch_size, channel, height, width)\n",
    "        tensor = self.output(tensor)\n",
    "\n",
    "        tensor = tensor + skip_tensor\n",
    "\n",
    "        # Feed Forward Layer\n",
    "        tensor = self.gnorm2(tensor)\n",
    "        tensor = self.ffn1(tensor)\n",
    "        tensor = F.relu(tensor)\n",
    "        tensor = self.ffn2(tensor)\n",
    "\n",
    "        return tensor\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, image_channel: int = 3, image_size: int = 64, channels: list[int] = [64, 128, 256, 512], attention_channels = [128, 256, 512], depth: int = 2, emb_dim: int = 1024, num_step: int = 1000, num_classes: int = 10, channel_per_group: int = 16, patch_size: int = 2, head_dim: int = 32):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.ModuleList([nn.ModuleList([nn.Conv2d(image_channel, channels[0], 3, padding=1)])])\n",
    "        self.decoder = nn.ModuleList()\n",
    "        skip_channel = [channels[0]]\n",
    "        image_size = [image_size // (2**i) for i in range(len(channels))]\n",
    "\n",
    "        self.positional_encoding = nn.Embedding(num_step, emb_dim)\n",
    "        self.positional_encoding.weight.data.copy_(positional_embedding(num_step, emb_dim))\n",
    "        self.positional_encoding.weight.requires_grad = False\n",
    "\n",
    "        # Encoder\n",
    "        for i in range(len(channels)):\n",
    "            for _ in range(depth):\n",
    "                layer = nn.ModuleList()\n",
    "                layer.append(ResBlock(channels[i], channels[i], emb_dim = emb_dim, channel_per_group=channel_per_group))\n",
    "                if channels[i] in attention_channels:\n",
    "                    layer.append(SelfAttentionBlock(channels[i], image_size[i]))\n",
    "                self.encoder.append(layer)\n",
    "                skip_channel.append(channels[i])\n",
    "\n",
    "            # Down projection\n",
    "            if i != len(channels)-1:\n",
    "                layer = nn.ModuleList()\n",
    "                layer.append(ResBlock(channels[i], channels[i + 1], down=True, emb_dim = emb_dim, channel_per_group=channel_per_group))\n",
    "                self.encoder.append(layer)\n",
    "                skip_channel.append(channels[i+1])\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottle_neck = nn.ModuleList([\n",
    "            ResBlock(channels[-1], channels[-1], channel_per_group=channel_per_group),\n",
    "            SelfAttentionBlock(channels[-1], image_size[-1]),\n",
    "            ResBlock(channels[-1], channels[-1], channel_per_group=channel_per_group),\n",
    "        ])\n",
    "\n",
    "        # Decoder\n",
    "        for i in range(len(channels)-1, -1, -1):\n",
    "            for _ in range(depth):\n",
    "                layer = nn.ModuleList()\n",
    "                layer.append(ResBlock(channels[i] + skip_channel.pop(), channels[i], emb_dim = emb_dim))\n",
    "                if channels[i] in attention_channels:\n",
    "                    layer.append(SelfAttentionBlock(channels[i], image_size[i]))\n",
    "                self.decoder.append(layer)\n",
    "\n",
    "            # Up projection\n",
    "            if i != 0:\n",
    "                layer = nn.ModuleList()\n",
    "                layer.append(ResBlock(channels[i] + skip_channel.pop(), channels[i - 1], up=True, emb_dim = emb_dim))\n",
    "                self.decoder.append(layer)\n",
    "            else:\n",
    "                layer = nn.ModuleList()\n",
    "                layer.append(ResBlock(channels[i] + skip_channel.pop(), channels[0], emb_dim = emb_dim))\n",
    "                self.decoder.append(layer)\n",
    "\n",
    "        self.time_embedding_proj = nn.Sequential(\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.class_embedding = nn.Embedding(num_classes, emb_dim)\n",
    "\n",
    "        # Output kernels to change back to image channel\n",
    "        self.out = nn.Sequential(\n",
    "            nn.GroupNorm(channels[0] // channel_per_group, channels[0], affine=False),\n",
    "            nn.SiLU(),\n",
    "            zero_out(nn.Conv2d(channels[0], image_channel, kernel_size = 1)),\n",
    "        )\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor, time_step: torch.Tensor, label: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Diffusion model.\n",
    "\n",
    "        :param tensor: Input tensor.\n",
    "        :param time_step: Time step tensor.\n",
    "        :param label: Label tensor.\n",
    "        :return: Predicted noise.\n",
    "        \"\"\"\n",
    "        embedding = self.positional_encoding(time_step)\n",
    "        embedding = self.time_embedding_proj(embedding)\n",
    "\n",
    "        if label != None:\n",
    "            class_embedding = self.class_embedding(label)\n",
    "            embedding = embedding + class_embedding\n",
    "\n",
    "        skip_connection = []\n",
    "\n",
    "        # Encoder\n",
    "        for layer in self.encoder:\n",
    "            for module in layer:\n",
    "                if(isinstance(module, ResBlock)):\n",
    "                    tensor = module(tensor, embedding)\n",
    "                else:\n",
    "                    tensor = module(tensor)\n",
    "\n",
    "            skip_connection.append(tensor)\n",
    "\n",
    "        # Bottleneck\n",
    "        for module in self.bottle_neck:\n",
    "            if(isinstance(module, ResBlock)):\n",
    "                tensor = module(tensor, embedding)\n",
    "            else:\n",
    "                tensor = module(tensor)\n",
    "\n",
    "        # Decoder\n",
    "        for layer in self.decoder:\n",
    "            tensor = torch.concatenate((tensor, skip_connection.pop()), dim = 1)\n",
    "            for module in layer:\n",
    "                if(isinstance(module, ResBlock)):\n",
    "                    tensor = module(tensor, embedding)\n",
    "                else:\n",
    "                    tensor = module(tensor)\n",
    "\n",
    "        tensor = self.out(tensor)\n",
    "\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m train_dataset, val_dataset \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mrandom_split(dataset, [train_size, val_size])\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Create DataLoaders for training and validation\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     25\u001b[0m valid_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataLoader' is not defined"
     ]
    }
   ],
   "source": [
    "# Define image transformations for preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "def valid_image_folder(path: str) -> bool:\n",
    "    # Check if file starts with '._' or ends with '.DS_Store'\n",
    "    filename = os.path.basename(path)\n",
    "    if filename.startswith(\"._\") or filename == \".DS_Store\": # Stupid MacOS\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Use ImageFolder to automatically label images based on folder names\n",
    "dataset = datasets.ImageFolder(root=dataset_filepath, is_valid_file=valid_image_folder, transform=transform)\n",
    "\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders for training and validation\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "9XB5rRMonmK0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model has 118823491 parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_106481/3785156192.py:4: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = amp.GradScaler()\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "unet = UNet(num_classes=num_class).cuda(device_id)\n",
    "print(\"This model has\", sum(p.numel() for p in unet.parameters()), \"parameters.\")\n",
    "scaler = amp.GradScaler()\n",
    "loss_train = []\n",
    "loss_valid = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "VTafC6Lennp3"
   },
   "outputs": [],
   "source": [
    "# Set up optimizer and loss\n",
    "optimizer = opt.AdamW(unet.parameters(), lr = lr, weight_decay = weight_decay)\n",
    "criterion = nn.MSELoss()\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs*len(train_dataloader), eta_min=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "WIUbXErUnrOA",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_106481/3122115251.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast():\n",
      "  0%|                                                                     | 0/1153050 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (128) must match the size of tensor b (32) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m amp\u001b[38;5;241m.\u001b[39mautocast():\n\u001b[0;32m---> 20\u001b[0m     predicted_noise \u001b[38;5;241m=\u001b[39m \u001b[43munet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(predicted_noise, noise)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Back propagation\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/Diffusion-CIFAR10/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/Diffusion-CIFAR10/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[5], line 237\u001b[0m, in \u001b[0;36mUNet.forward\u001b[0;34m(self, tensor, time_step, label)\u001b[0m\n\u001b[1;32m    235\u001b[0m             tensor \u001b[38;5;241m=\u001b[39m module(tensor, embedding)\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 237\u001b[0m             tensor \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m     skip_connection\u001b[38;5;241m.\u001b[39mappend(tensor)\n\u001b[1;32m    241\u001b[0m \u001b[38;5;66;03m# Bottleneck\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/Diffusion-CIFAR10/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/Diffusion-CIFAR10/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[5], line 107\u001b[0m, in \u001b[0;36mSelfAttentionBlock.forward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# Reshape for self attention\u001b[39;00m\n\u001b[1;32m    106\u001b[0m batch_size, channel, height, width \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m--> 107\u001b[0m tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpositional_encoding\u001b[49m\n\u001b[1;32m    108\u001b[0m tensor \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mview(batch_size, channel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_pixel)\n\u001b[1;32m    109\u001b[0m tensor \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (128) must match the size of tensor b (32) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    train_loss_list = []\n",
    "    valid_loss_list = []\n",
    "    for images, label in tqdm(train_dataloader):\n",
    "        # Zero out grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Preparing for forward pass\n",
    "        images = images.cuda(device_id)\n",
    "        label = label.cuda(device_id)\n",
    "        time_step = torch.randint(1, steps, size = (images.shape[0], )).cuda(device_id)\n",
    "        x_t, noise = forward_pass(images, time_step)\n",
    "\n",
    "        # Classifier free guidance.\n",
    "        if random.random() < 0.1:\n",
    "            label = None\n",
    "\n",
    "        # Forward pass\n",
    "        with amp.autocast():\n",
    "            predicted_noise = unet(x_t, time_step, label)\n",
    "            loss = criterion(predicted_noise, noise)\n",
    "\n",
    "        # Back propagation\n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(unet.parameters(), max_norm=1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Record loss\n",
    "        train_loss_list.append(loss.item())\n",
    "\n",
    "        # Step the learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "    if(epoch % 10 == 0):\n",
    "        with torch.no_grad():\n",
    "            for images, label in tqdm(valid_dataloader):\n",
    "                # Preparing for forward pass\n",
    "                images = images.cuda(device_id)\n",
    "                label = label.cuda(device_id)\n",
    "                time_step = torch.randint(1, steps, size = (images.shape[0], )).cuda(device_id)\n",
    "                x_t, noise = forward_pass(images, time_step)\n",
    "\n",
    "                # Forward pass\n",
    "                with amp.autocast():\n",
    "                    predicted_noise = unet(x_t, time_step, label)\n",
    "                    loss = criterion(predicted_noise, noise)\n",
    "                valid_loss_list.append(loss.item())\n",
    "\n",
    "    print(f\"Epoch #{epoch}\")\n",
    "    print(f\"Current learning rate is {optimizer.param_groups[0]['lr']}\")\n",
    "    print(\"Train Loss is:\", sum(train_loss_list)/len(train_loss_list))\n",
    "    loss_train.append(sum(train_loss_list)/len(train_loss_list))\n",
    "    if(epoch % 10 == 0):\n",
    "        print(\"Valid Loss is:\", sum(valid_loss_list)/len(valid_loss_list))\n",
    "        loss_valid.append(sum(valid_loss_list)/len(valid_loss_list))\n",
    "    if(epoch % 10 == 0):\n",
    "        label = torch.tensor([0, 1]).cuda(device_id)\n",
    "        sampling(unet, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "uhi1Rx1Y1GE5"
   },
   "outputs": [],
   "source": [
    "torch.save(unet, \"diffusion_CIFAR10.pth\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "09a84bd1dc98317f17d8982ec5713d832b330e2f9a95d130d3bd5924dbe90d6c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
